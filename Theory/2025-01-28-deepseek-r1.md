---
title:  "R1: The AlphaGo Zero Moment for Large Language Models?*"
date:   2025-01-28 10:00:00 +0800
categories: [Theory]
---

The world of Large Language Models (LLMs) is constantly evolving, and the latest advancements are truly mind-bending. We've seen impressive capabilities emerge, but one of the most crucial areas of development has been pushing these models to engage in *deep thinking* – true, multi-step reasoning. For a while, the go-to method was Chain-of-Thought (CoT) prompting. But now, there's a new approach shaking things up: R1.

**The Evolution of LLM Reasoning: From CoT to MCTS**

Let's quickly recap how we've been trying to get LLMs to reason deeply. The traditional method, CoT, involves getting the LLM to break down a problem into a series of intermediate steps. This is like building a chain of logic, and it's been a big improvement over simply asking for the final answer.

Then came more sophisticated strategies, like **LATS**, which uses the concept of Monte Carlo Tree Search (MCTS).  Think of it like this: each step in the chain of reasoning becomes a node in a tree. The LLM explores multiple paths (chains of thought), scores each one, and ultimately selects the path with the highest overall score. It's a bit like how AlphaGo evaluates board positions – predicting future moves to determine the best action. LATS, powered by LangGraph, effectively brings the power of MCTS to COT. The best part is, you can plug and play with any LLM and see an immediate improvement. It's a powerful methodology, rather than being bound to a single LLM!

**The GPT-o1 Approach: The High-Cost Textbook**

Before we move onto R1, it's worth mentioning another approach, reportedly used by the mysterious project q* (now associated with GPT-o1). Instead of letting an LLM generate multiple reasoning chains and then score them, q* relies on *human-written* reasoning steps. Essentially, they're creating textbook-quality, step-by-step guides to ensure top-notch CoT results. This high-cost, labor-intensive approach is like having humans meticulously transcribe all top chess matches for AlphaGo, rather than letting the system learn through self-play. This also might be the main reason that Open AI is very reluctant to publish more details about this technology

**Introducing R1: Unleashing Exploration**

Now for the really exciting part: R1.  This method takes a completely different approach.  It *abandons the idea of pre-defined steps or a fixed tree structure*. Instead, **R1 lets the LLM explore a much wider range of reasoning possibilities**. It's like stepping back and giving the model the freedom to experiment.

And what happens when you do that?  Incredibly, **R1 leads to the discovery of highly creative and novel reasoning pathways** that wouldn't be seen using the more rigid CoT/MCTS approach. It’s like AlphaGo Zero, which after enough self-play was able to discover new strategies and move combinations that human professionals had never conceived of. It turns out that sometimes freedom yields better results.

Just like LATS is LLM agnostic, R1 can also be used with any base LLM. Researchers who have replicated R1 consistently find that the better the underlying LLM, the more impressive the reasoning it can discover. R1 is not a model, but a way of making models generate better outputs.

**A New Era for LLMs**

This is truly a "zero moment" for LLMs, echoing the paradigm shift in reinforcement learning we saw with AlphaGo. We're moving beyond the limitations of pre-programmed thought processes and towards a future where LLMs can truly think for themselves, in creative and insightful ways. I, for one, am extremely excited to see where this leads!

**What do you think about the potential of R1? Share your thoughts in the comments below!**
